title: "UD_Tagalog-NewsCrawl (Experiments)"

directories:
  - "assets"
  - "corpus"
  - "configs"
  - "packages"
  - "metrics"
  - "metrics/trg"
  - "metrics/ugnayan"
  - "metrics/xlingual"
  - "pretraining"
  - "training"
  - "vectors"

vars:
  pretrain_epochs: 3
  gpu_id: 0
  # For package-and-upload command
  model_path: ""
  model_name: ""
  hf_user: "UD-Filipino"
  treebank_name: ""
  lang: "tl"

workflows:
  setup:
    - "setup-training-data"
    - "setup-fasttext-vectors"
    - "build-floret"
  baseline-transition:
    - "train-spacy-parser"
  fasttext-transition:
    - "train-spacy-parser-fasttext"
  hash-transition:
    - "train-floret-vectors"
    - "train-spacy-parser-floret"
  xling-transition:
    - "train-spacy-parser-trf-xling"
  mono-transition:
    - "train-spacy-parser-trf-mono"
  evaluate:
    - "evaluate-spacy"
    - "evaluate-old"

assets:
  - dest: assets/tl_newscrawl-ud-train.conllu
    description: "Train dataset for NewsCrawl"
    url: https://raw.githubusercontent.com/UniversalDependencies/UD_Tagalog-NewsCrawl/refs/heads/dev/tl_newscrawl-ud-train.conllu
  - dest: assets/tl_newscrawl-ud-dev.conllu
    description: "Dev dataset for NewsCrawl"
    url: https://raw.githubusercontent.com/UniversalDependencies/UD_Tagalog-NewsCrawl/refs/heads/dev/tl_newscrawl-ud-dev.conllu
  - dest: assets/tl_newscrawl-ud-test.conllu
    description: "Test dataset for NewsCrawl"
    url: https://raw.githubusercontent.com/UniversalDependencies/UD_Tagalog-NewsCrawl/refs/heads/dev/tl_newscrawl-ud-test.conllu
  - dest: assets/tl_trg-ud-test.conllu
    description: "Test dataset for TRG"
    url: https://raw.githubusercontent.com/UniversalDependencies/UD_Tagalog-TRG/refs/heads/master/tl_trg-ud-test.conllu
  - dest: assets/tl_ugnayan-ud-test.conllu
    description: "Test dataset for Ugnayan"
    url: https://raw.githubusercontent.com/UniversalDependencies/UD_Tagalog-Ugnayan/refs/heads/master/tl_ugnayan-ud-test.conllu
  - dest: "assets/fasttext.tl.gz"
    description: "Tagalog fastText vectors provided from the fastText website (trained from CommonCrawl and Wikipedia)."
    url: "https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tl.300.vec.gz"
  - dest: "assets/floret"
    description: "Floret repository for training floret and fastText models."
    git:
      repo: "https://github.com/explosion/floret"
      branch: "main"
      path: ""
  - dest: "assets/tlunified_raw_text.jsonl"
    description: "Pre-converted raw text from TLUnified in JSONL format (1.1 GB)."
    url: "https://storage.googleapis.com/ljvmiranda/calamanCy/tlunified_raw_text.jsonl"
  - dest: "assets/tlunified_raw_text.txt"
    description: "Pre-converted raw text from TLUnified in JSONL format (1.1 GB)."
    url: "https://storage.googleapis.com/ljvmiranda/calamanCy/tlunified_raw_text.txt"
    # Cross-lingual experiments
  - dest: assets/id_gsd-ud-train.conllu
    description: "Indonesian UD-GSD Treebank Train Set"
    url: https://raw.githubusercontent.com/UniversalDependencies/UD_Indonesian-GSD/refs/heads/master/id_gsd-ud-train.conllu
  - dest: assets/id_gsd-ud-dev.conllu
    description: "Indonesian UD-GSD Treebank Dev Set"
    url: https://raw.githubusercontent.com/UniversalDependencies/UD_Indonesian-GSD/refs/heads/master/id_gsd-ud-dev.conllu
  - dest: assets/uk_iu-ud-train.conllu
    description: "Ukrainian UD-IU Treebank Train Set"
    url: https://raw.githubusercontent.com/UniversalDependencies/UD_Ukrainian-IU/refs/heads/master/uk_iu-ud-train.conllu
  - dest: assets/uk_iu-ud-dev.conllu
    description: "Ukrainian UD-IU Treebank Dev Set"
    url: https://raw.githubusercontent.com/UniversalDependencies/UD_Ukrainian-IU/refs/heads/master/uk_iu-ud-dev.conllu
  - dest: assets/ro_rrt-ud-train.conllu
    description: "Romanian UD-RRT Treebank Train Set"
    url: https://raw.githubusercontent.com/UniversalDependencies/UD_Romanian-RRT/refs/heads/master/ro_rrt-ud-train.conllu
  - dest: assets/ro_rrt-ud-dev.conllu
    description: "Romanian UD-RRT Treebank Dev Set"
    url: https://raw.githubusercontent.com/UniversalDependencies/UD_Romanian-RRT/refs/heads/master/ro_rrt-ud-dev.conllu
  - dest: assets/vi_vtb-ud-train.conllu
    description: "Vietnamese UD-VTB Treebank Train Set"
    url: https://raw.githubusercontent.com/UniversalDependencies/UD_Vietnamese-VTB/refs/heads/master/vi_vtb-ud-train.conllu
  - dest: assets/vi_vtb-ud-dev.conllu
    description: "Vietnamese UD-VTB Treebank Dev Set"
    url: https://raw.githubusercontent.com/UniversalDependencies/UD_Vietnamese-VTB/refs/heads/master/vi_vtb-ud-dev.conllu
  - dest: assets/ca_ancora-ud-train.conllu
    description: "Catalan UD-AnCora Treebank Train Set"
    url: https://raw.githubusercontent.com/UniversalDependencies/UD_Catalan-AnCora/refs/heads/master/ca_ancora-ud-train.conllu
  - dest: assets/ca_ancora-ud-dev.conllu
    description: "Catalan UD-AnCora Treebank Dev Set"
    url: https://raw.githubusercontent.com/UniversalDependencies/UD_Catalan-AnCora/refs/heads/master/ca_ancora-ud-dev.conllu

commands:
  - name: "setup-training-data"
    help: "Prepare Tagalog corpora for training and evaluating the parser"
    script:
      - >-
        python -m spacy convert 
        assets/tl_newscrawl-ud-train.conllu corpus/
        --converter conllu
        --morphology
        --merge-subtokens
      - >-
        python -m spacy convert 
        assets/tl_newscrawl-ud-dev.conllu corpus/
        --converter conllu
        --morphology
        --merge-subtokens
      - >-
        python -m spacy convert 
        assets/tl_newscrawl-ud-test.conllu corpus/
        --converter conllu
        --n-sents 1
        --morphology
        --merge-subtokens
      - >-
        python -m spacy convert 
        assets/tl_ugnayan-ud-test.conllu corpus/
        --converter conllu
        --n-sents 1
        --morphology
        --merge-subtokens
      - >-
        python -m spacy convert 
        assets/tl_trg-ud-test.conllu corpus/
        --converter conllu
        --n-sents 1
        --morphology
        --merge-subtokens
    deps:
      - assets/tl_newscrawl-ud-train.conllu
      - assets/tl_newscrawl-ud-dev.conllu
      - assets/tl_newscrawl-ud-test.conllu
      - assets/tl_ugnayan-ud-test.conllu
      - assets/tl_trg-ud-test.conllu
    outputs:
      - corpus/tl_newscrawl-ud-train.spacy
      - corpus/tl_newscrawl-ud-dev.spacy
      - corpus/tl_newscrawl-ud-test.spacy
      - corpus/tl_ugnayan-ud-test.spacy
      - corpus/tl_trg-ud-test.spacy

  - name: "setup-fasttext-vectors"
    help: "Make fastText vectors spaCy compatible"
    script:
      - gzip -d -f assets/fasttext.tl.gz
      - mkdir -p vectors/fasttext-tl
      - >-
        python -m spacy init vectors
        tl assets/fasttext.tl vectors/fasttext-tl
    deps:
      - assets/fasttext.tl.gz
    outputs:
      - vectors/fasttext-tl

  - name: "build-floret"
    help: "Build floret binary for training fastText / floret vectors"
    script:
      - make -C assets/floret
      - chmod +x assets/floret/floret
    deps:
      - assets/floret
    outputs:
      - assets/floret/floret

  - name: "train-floret-vectors"
    help: "Train floret word vectors (200 dims, 200k keys)"
    script:
      - mkdir -p assets/vectors/floret-tl/
      - >-
        assets/floret/floret skipgram
        -input assets/tlunified_raw_text.txt
        -output assets/vectors/floret-tl/vectors
        -dim 200
        -minn 3
        -maxn 5
        -mode floret
        -hashCount 2
        -bucket 200000
      - mkdir -p vectors/floret
      - >-
        python -m spacy init vectors
        tl assets/vectors/floret-tl/vectors.floret vectors/floret-tl
        --mode floret
    deps:
      - assets/floret/floret
      - assets/tlunified_raw_text.txt
    outputs:
      - vectors/floret-tl

  # - name: "pretrain"
  #   help: "Pretrain with information from raw text"
  #   script:
  #     - >-
  #       python -m spacy pretrain configs/parser.cfg pretraining/
  #       --paths.raw_text assets/tlunified_raw_text.jsonl
  #       --pretraining.max_epochs ${vars.pretrain_epochs}
  #       --pretraining.n_save_every 1
  #       --gpu-id ${vars.gpu_id}
  #   deps:
  #     - assets/tlunified_raw_text.jsonl
  #   outputs:
  #     - pretraining/model-last.bin

  - name: "train-spacy-parser"
    help: "Train parser, tagger, and morph using the UD-NewsCrawl"
    script:
      - >-
        python -m spacy train
        configs/parser.cfg
        --output training/spacy_baseline/
        --nlp.lang tl
        --paths.train corpus/tl_newscrawl-ud-train.spacy
        --paths.dev corpus/tl_newscrawl-ud-dev.spacy
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/tl_newscrawl-ud-train.spacy
      - corpus/tl_newscrawl-ud-dev.spacy
    outputs:
      - training/spacy_baseline/model-best

  - name: "train-spacy-parser-floret"
    help: "Train parser, tagger, and morph using the UD-NewsCrawl"
    script:
      - >-
        python -m spacy train
        configs/parser.cfg
        --output training/hash_transition/
        --nlp.lang tl
        --paths.train corpus/tl_newscrawl-ud-train.spacy
        --paths.dev corpus/tl_newscrawl-ud-dev.spacy
        --paths.vectors vectors/floret-tl
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/tl_newscrawl-ud-train.spacy
      - corpus/tl_newscrawl-ud-dev.spacy
    outputs:
      - training/hash_transition/model-best

  - name: "train-spacy-parser-fasttext"
    help: "Train parser, tagger, and morph using the UD-NewsCrawl"
    script:
      - >-
        python -m spacy train
        configs/parser.cfg
        --output training/fasttext_transition/
        --nlp.lang tl
        --paths.train corpus/tl_newscrawl-ud-train.spacy
        --paths.dev corpus/tl_newscrawl-ud-dev.spacy
        --paths.vectors vectors/fasttext-tl
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/tl_newscrawl-ud-train.spacy
      - corpus/tl_newscrawl-ud-dev.spacy
    outputs:
      - training/fasttext_transition/model-best

  - name: "train-spacy-parser-trf-xling"
    help: "Train parser, tagger, and morph using the UD-NewsCrawl"
    script:
      - >-
        python -m spacy train
        configs/parser_trf.cfg
        --output training/xling-transition/
        --components.transformer.model.name FacebookAI/xlm-roberta-large
        --paths.train corpus/tl_newscrawl-ud-train.spacy
        --paths.dev corpus/tl_newscrawl-ud-dev.spacy
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/tl_newscrawl-ud-train.spacy
      - corpus/tl_newscrawl-ud-dev.spacy
    outputs:
      - training/xling-transition/model-best

  - name: "train-spacy-parser-trf-xling2"
    help: "Train parser, tagger, and morph using the UD-NewsCrawl"
    script:
      - >-
        python -m spacy train
        configs/parser_trf.cfg
        --output training/xling-transition-2/
        --components.transformer.model.name microsoft/mdeberta-v3-base
        --paths.train corpus/tl_newscrawl-ud-train.spacy
        --paths.dev corpus/tl_newscrawl-ud-dev.spacy
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/tl_newscrawl-ud-train.spacy
      - corpus/tl_newscrawl-ud-dev.spacy
    outputs:
      - training/xling-transition-2/model-best

  - name: "train-spacy-parser-trf-mono"
    help: "Train parser, tagger, and morph using the UD-NewsCrawl"
    script:
      - >-
        python -m spacy train
        configs/parser_trf.cfg
        --output training/mono-transition/
        --components.transformer.model.name jcblaise/roberta-tagalog-large
        --paths.train corpus/tl_newscrawl-ud-train.spacy
        --paths.dev corpus/tl_newscrawl-ud-dev.spacy
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/tl_newscrawl-ud-train.spacy
      - corpus/tl_newscrawl-ud-dev.spacy
    outputs:
      - training/mono-transition/model-best

  - name: "evaluate-spacy"
    help: "Evaluate a spaCy model uploaded to HuggingFace to the test split"
    script:
      - >-
        python -m pip install 
        https://huggingface.co/${vars.hf_user}/${vars.model_name}/resolve/main/${vars.model_name}-any-py3-none-any.whl
      - >-
        python -m spacy evaluate
        ${vars.model_name} corpus/tl_newscrawl-ud-test.spacy
        --output metrics/${vars.model_name}.json
        --gpu-id ${vars.gpu_id}
        --per-component
    deps:
      - corpus/tl_newscrawl-ud-test.spacy
    outputs:
      - metrics/${vars.model_name}.json

  - name: "evaluate-old"
    help: "Evaluate a spaCy model uploaded to HuggingFace to previous treebanks"
    script:
      - >-
        python -m pip install 
        https://huggingface.co/${vars.hf_user}/${vars.model_name}/resolve/main/${vars.model_name}-any-py3-none-any.whl
      - >-
        python -m spacy evaluate
        ${vars.model_name} corpus/tl_ugnayan-ud-test.spacy
        --output metrics/ugnayan/${vars.model_name}.json
        --gpu-id ${vars.gpu_id}
        --per-component
      - >-
        python -m spacy evaluate
        ${vars.model_name} corpus/tl_trg-ud-test.spacy
        --output metrics/trg/${vars.model_name}.json
        --gpu-id ${vars.gpu_id}
        --per-component
    deps:
      - corpus/tl_ugnayan-ud-test.spacy
      - corpus/tl_trg-ud-test.spacy
    outputs:
      - metrics/ugnayan/${vars.model_name}.json
      - metrics/trg/${vars.model_name}.json

  - name: "train-and-eval-xlingual"
    help: "Convert a treebank to spaCy format, train, and evaluate on the test set of UD-NewsCrawl"
    script:
      - >-
        python -m spacy convert 
        assets/${vars.treebank_name}-train.conllu corpus/
        --converter conllu
        --morphology
        --merge-subtokens
      - >-
        python -m spacy convert 
        assets/${vars.treebank_name}-dev.conllu corpus/
        --converter conllu
        --morphology
        --merge-subtokens
      - >-
        python -m spacy train
        configs/parser.cfg
        --output training/xlingual-${vars.treebank_name}
        --nlp.lang ${vars.lang}
        --paths.train corpus/${vars.treebank_name}-train.spacy
        --paths.dev corpus/${vars.treebank_name}-dev.spacy
        --gpu-id ${vars.gpu_id}
      - >-
        python -m spacy evaluate
        training/xlingual-${vars.treebank_name}/model-best corpus/tl_newscrawl-ud-test.spacy
        --output metrics/xlingual/${vars.treebank_name}.json
        --gpu-id ${vars.gpu_id}
        --per-component
    deps:
      - assets/${vars.treebank_name}-train.conllu
      - assets/${vars.treebank_name}-dev.conllu
      - corpus/tl_newscrawl-ud-test.spacy
    outputs:
      - training/xlingual-${vars.treebank_name}/model-best
      - metrics/xlingual/${vars.treebank_name}.json

  - name: "package-and-upload"
    help: "Package a spaCy pipeline and upload to HuggingFace"
    script:
      - python -m spacy package ${vars.model_path} packages --build sdist,wheel --name ${vars.model_name} --meta ./meta.json --force
      - python -m spacy huggingface-hub push packages/tl_${vars.model_name}-0.0.0/dist/tl_${vars.model_name}-0.0.0-py3-none-any.whl --verbose --org UD-Filipino
    deps:
      - ${vars.model_path}
